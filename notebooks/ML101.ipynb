{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook objectives\n",
    "* Define machine learning\n",
    "* Discuss estimation equations and cost/loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is machine learning according to a Google search?  \n",
    "\n",
    "\n",
    "<br>\n",
    "<font color=\"red\" size=\"4\">**Does this image increase your understanding of ML?**</font>\n",
    "\n",
    "<img src=\"images/ml_brain.jpg\" align=\"left\"></img> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning is estimating and minimizing error\n",
    "\n",
    "<img src=\"images/lin_reg.jpg\" width=700 align=\"right\"></img>\n",
    "\n",
    "**x** = independent variable   \n",
    "**y** = dependent or target variable  \n",
    "**^** used to denote an estimate  \n",
    "**-** used to denote the mean\n",
    "\n",
    "### Possibly familiar format\n",
    "* Estimation equation:  $\\hat{y} = mx + b$\n",
    "* Scoring:  $(y - \\hat{y})^2$ \n",
    "\n",
    "### Machine learning format \n",
    "* Estimation equation:  $\\hat{y} = \\beta_0 + \\beta_1 x_1...$\n",
    "* Cost or loss function: $(y - \\hat{y})^2$\n",
    "\n",
    "<br>\n",
    "<font color=\"red\" size=\"4\">**Why square the difference?**</font> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some common syntax\n",
    "\n",
    "* Scalar is a number\n",
    "* Vector is a collection of scalars that is 1-dimensional (a sequence of scalars)\n",
    "    * Typically represented with a lowercase letter such as y\n",
    "* Matix is a collection of vectors that generally 2-dimensional (a table of data)\n",
    "    * Typically represented with an uppercase letter such as X\n",
    "    * The rows or columns of the matrix are vectors  \n",
    "\n",
    "<br>\n",
    "<font color=\"red\" size=\"4\">**Which one of these terms generally describes the target variable?**</font>\n",
    "<br>\n",
    "<font color=\"red\" size=\"4\">**Which one of these terms generally describes the data for your model?**</font>\n",
    "<br>\n",
    "<br>\n",
    "<img src=\"images/scalar-vector-matrix.svg\" align=\"left\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Types of machine learning\n",
    "\n",
    "<img src=\"images/ml_types.jpg\" align=\"right\"></img>\n",
    "\n",
    "### Supervised learning (y is known)\n",
    "#### Regression (y is continuous)\n",
    "* Linear models (linear, polynomial, Poisson regressions, etc)\n",
    "* Time series forecasting (autoregression)\n",
    "* Tree models (decision trees, random forests, boosted trees)  \n",
    "\n",
    "#### Classification (y represents a class)\n",
    "* K-nearest neighbors\n",
    "* Logistic regression\n",
    "* Support vector machines\n",
    "* Tree models (decision trees, random forests, boosted trees)\n",
    "* Neural networks (deep learning)\n",
    "\n",
    "### Unsupervised learning (y is unknown)\n",
    "#### Clustering (creates groups and assigns y)\n",
    "* K-means (groups by distance from centroids)\n",
    "* Hierarchical (groups based on similarity)\n",
    "* DBSCAN (groups based upon density)\n",
    "\n",
    "<br>\n",
    "<font color=\"red\" size=\"4\">**What other machine learning algorithsms missing from the list?**</font>  \n",
    "<br>\n",
    "<font color=\"red\" size=\"4\">**What are some business that make their money off regression? How about classification?**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data\n",
    "Original data obtained from obtained from [Seattle Real Time Fire 911 Calls](https://data.seattle.gov/Public-Safety/Seattle-Real-Time-Fire-911-Calls/kzjm-xkqj). The [Seattle Open Data Portal](https://data.seattle.gov/browse?limitTo=datasets) has a bunch of fun real (and sometimes messy) data to play with.\n",
    "\n",
    "Features were created and data was aggegrated to generate the dataset found in the data directory. This is the same data that powers my project [Rapid-Rescue](http://rapid-rescue.com/) that uses Poisson regression and K-means clustering to estimate number of medical emergencies in each area of Seattle and optimal placements for emergency response vehicles given historical data.\n",
    "\n",
    "<br>\n",
    "<font color=\"red\" size=\"4\">**What are some interesting pieces of data to study here at Nordstrom?**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data, check for nulls, ensure it's properly cast, and create X and y\n",
    "df = pd.read_csv('../data/model_data.csv', index_col=0)\n",
    "data_cols = [col for col in df.columns if col != 'freq']\n",
    "X = df[data_cols]\n",
    "y = df.freq\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at first few rows\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data into training and tests sets\n",
    "\n",
    "* The training set is used to train the model\n",
    "* The test data set is used to validate the model and explore its generalizability\n",
    "\n",
    "<br>\n",
    "<font color=\"red\" size=\"4\">**Why is it important to split the data?**</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=97)\n",
    "\n",
    "depths = xrange(1,36)\n",
    "train_scores, test_scores = [], []\n",
    "\n",
    "for depth in depths:\n",
    "    DTR = DecisionTreeRegressor(max_depth=depth)\n",
    "    DTR.fit(X_train, y_train)\n",
    "    train_scores.append(-DTR.score(X_train, y_train))\n",
    "    test_scores.append(-DTR.score(X_test, y_test))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(depths, train_scores)\n",
    "plt.ylabel('Error')\n",
    "plt.xlabel('Model Complexity')\n",
    "plt.title('Decision Tree Regressor Train Error');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(depths, train_scores, label='train')\n",
    "plt.scatter(depths, test_scores, label='test')\n",
    "plt.ylabel('Error')\n",
    "plt.xlabel('Model Complexity')\n",
    "plt.title('Decision Tree Regressor Train and Test Error')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Bias/variance trade off and under/overfitting\n",
    "\n",
    "* Bias refers to the error introduced by approximating a complex real-world problem with a simpler model (rigidity and relationship assumptions)\n",
    "* Variance describes the degree to which the estimation equation will vary if trained on a different dataset (flexibility)\n",
    "\n",
    "If the model has overly high bias, it will be too inflexible to effectively approximate the data. This type of model is **underfit** (see below left). If the model has overly high variance, it will not be generalizable to other data. When the model fits the training data, but is not generalizable to other data, it is said to be **overfit** (see below right).\n",
    "\n",
    "[Matt Drury](https://github.com/madrury) created a fantastic [visualization tool](http://madrury.github.io/smoothers/) for bias and variance.\n",
    "\n",
    "<br>\n",
    "<font color=\"red\" size=\"4\">**How can we determine if a model is underfit? **</font>  \n",
    "<br>\n",
    "<font color=\"red\" size=\"4\">**How can we determine if a model is overfit?**</font>\n",
    "\n",
    "\n",
    "<img src=\"images/bias_variance.png\" align=\"left\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# What are hyperparameters?\n",
    "\n",
    "* Parameters of the model that are not directly \"learned\" from the data\n",
    "* Generally set before the model is run\n",
    "* Optimized through trial and error\n",
    "* Often controls bias/variance trade off for model\n",
    "\n",
    "<br>\n",
    "<font color=\"red\" size=\"4\">**Wouldn't it be great if there was an easy way to test hyperparameters?**</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Grid searching and cross-validation\n",
    "\n",
    "* K-fold cross validation breaks training data into K training sets and K test sets\n",
    "* Tools in Python and R for automating the process of use cross validation to test combinations of hyperparemeters \n",
    "\n",
    "<br>\n",
    "<font color=\"red\" size=\"4\">**Why is it desirable to create more training sets?**</font>  \n",
    "<br>\n",
    "<font color=\"red\" size=\"4\">**What problems might this help us spot?**</font>\n",
    "\n",
    "<img src=\"images/kfold.gif\" align=\"left\"></img>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate model and set hyperparaters to test, and instantiate GridSearchCV\n",
    "DTR = DecisionTreeRegressor()\n",
    "param_grid = {'max_depth': range(1, 35)}\n",
    "GSCV = GridSearchCV(estimator=DTR,\n",
    "                    param_grid=param_grid,\n",
    "                    n_jobs=-1,\n",
    "                    cv=3,\n",
    "                    return_train_score=True)\n",
    "\n",
    "GSCV.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(GSCV.cv_results_.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "x_vals = range(1, 35)\n",
    "plt.scatter(x_vals, -GSCV.cv_results_['split0_test_score'], label='split0_test_score')\n",
    "plt.scatter(x_vals, -GSCV.cv_results_['split1_test_score'], label='split1_test_score')\n",
    "plt.scatter(x_vals, -GSCV.cv_results_['split2_test_score'], label='split2_test_score')\n",
    "plt.scatter(x_vals, -GSCV.cv_results_['split0_train_score'], label='split0_train_score')\n",
    "plt.scatter(x_vals, -GSCV.cv_results_['split1_train_score'], label='split1_train_score')\n",
    "plt.scatter(x_vals, -GSCV.cv_results_['split2_train_score'], label='split2_train_score')\n",
    "plt.ylabel('Error')\n",
    "plt.xlabel('Model Complexity')\n",
    "plt.title('Decision Tree Regressor Train and Test Error')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Let's see what happens with a data issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make some random data and add it to our training data\n",
    "y_bad = np.random.randint(0, 100, 3000)\n",
    "X_bad = np.random.rand(3000, 21)\n",
    "y_train_bad = np.append(y_train, y_bad)\n",
    "X_train_bad = np.append(X_train, X_bad, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GSCV.fit(X_train_bad, y_train_bad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "x_vals = range(1, 35)\n",
    "plt.scatter(x_vals, -GSCV.cv_results_['split0_test_score'], label='split0_test_score')\n",
    "plt.scatter(x_vals, -GSCV.cv_results_['split1_test_score'], label='split1_test_score')\n",
    "plt.scatter(x_vals, -GSCV.cv_results_['split2_test_score'], label='split2_test_score')\n",
    "plt.scatter(x_vals, -GSCV.cv_results_['split0_train_score'], label='split0_train_score')\n",
    "plt.scatter(x_vals, -GSCV.cv_results_['split1_train_score'], label='split1_train_score')\n",
    "plt.scatter(x_vals, -GSCV.cv_results_['split2_train_score'], label='split2_train_score')\n",
    "plt.ylabel('Error')\n",
    "plt.xlabel('Model Complexity')\n",
    "plt.title('Decision Tree Regressor Train and Test Error')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
